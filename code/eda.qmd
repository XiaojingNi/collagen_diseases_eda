---
title: "Exploratory data analysis"
subtitle: "for collagen diseases dataset"
author:
  - name: Xiaojing Ni
    affiliations:
      - name: Georgetown University        
date: today
format:
    html:
        toc: true
        embed-resources: true
        theme: default
        code-copy: true
        code-line-numbers: true
        number-sections: true
        highlight-style: github
        link-external-icon: true
        link-external-newwindow: true
---
## Data summary
In order to explore the mechanisms of collagen diseases, a dataset is extracted from a University hospital database. The overall objective is to study potential factors helping to detect and predict thrombosis, one of the severe complications cased by collagen diseases. The dataset contains three parts: users' information and diagnosis, examination results of users' having thrombosis, and general laboratory examination results. <br>

The examination related to thrombosis mainly through the blood test. Thus, the dataset describing examination results of users' having thrombosis contains anti-Cardiolipin antibody measurement and degree of coagulation measurement (the action or process of blood changing to a solid or semi-solid state), along with the degree of thrombosis. The general laboratory examination results include general blood test, such as Red blood cell count, blood glucose, and total bilirubin. Those tests are not necessary relate to diagnosed thrombosis and can happen anytime when doctors think the patient need them. The three datasets are connected by patient ID. The datasets are one to many relation. For example, one patient can have various tests on same or different date. And patients can also have one or more diagnosis. 

## Initial questions
The initial questions are listed below. 
<ul>  
<li> What are the symptoms causing the doctor suspect a patient is having a thrombosis?
<li> Are some of the measurements correlated to each other? 
<li> Are some of the symptoms always occurring together? 
</ul>

## Data munging
```{r}
#| echo: false
#| warning: false
### this cell is used to run python in qmd so that one notebook can run both r and python
library(reticulate)
use_python("/Users/xiaojingni/miniforge3/envs/anly503/bin/python")
```
```{python}
# | echo: false
# | warning: false
# import packages
from dateutil import relativedelta
from datetime import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import collections
import seaborn as sns
```

### TSUMOTO_A
#### load patient data
Some feature meaning
<ul>
<li>Description: the date when a patient was input
<li>First date: the date when a patient came to the hospital
<li>Admission: patient was admitted to the hospital or followed at the outpatient clinic
<li>Diagnosis: some patients may suffer from several diseases
</ul>

```{python}
# read data
patients_df_raw = pd.read_csv(
    "../data/TSUMOTO_A.csv", encoding='windows-1252')

```
#### Missing values and duplicates
```{python}
# basic info of the raw data
patients_df_raw
patients_df_raw.info()
```
There are some missing values. The missing values are shown below. All of the records have a Id and Diagnosis. Birthday has one missing value. There are 248 patient without the "First Date" information (the date when a patient came to the hospital). 
```{python}
patients_df_raw.isna().sum()
```
Check if ID column has duplicate value
```{python}
# check if ID have duplicate value
if len(patients_df_raw) != len(set(patients_df_raw.ID)):
    print("duplicates found in the list")
else:
    print("No duplicates found in the list")
```
List duplicate ID and their records
```{python}
# below code is revised from https://www.trainingint.com/how-to-find-duplicates-in-a-python-list.html

id_set = set()
dupset = set()
for i in patients_df_raw.ID:
    if i not in id_set:
        id_set.add(i)
    else:
        # this method catches the first duplicate entries, and appends them to the list
        dupset.add(i)
# The next step is to print the duplicate entries, and the unique entries
print("List of duplicates\n",
      patients_df_raw.loc[patients_df_raw['ID'].isin(list(dupset))])
```
For patient 2557319, only one record has full data. Thus, the duplicate is probably from a bad entry. Delete one record will solve the problem. For patient 5807039, the first date is the same, but the date input is not, keeping the one with early input. 

```{python}
patients_df_raw = patients_df_raw.drop(labels=[264, 1215], axis=0)

# sanity check
# check if ID have duplicate value
if len(patients_df_raw) != len(set(patients_df_raw.ID)):
    print("duplicates found in the list")
else:
    print("No duplicates found in the list")
```
#### Cleaning and data format
Some of the date entries have type, replacing the letter with "" with solve this. Below I created three new column to change the format of the date columns: "Birthday", "Description", and "First Date". 
```{python}
# reformat
# Birthday
patients_df_raw['b-day'] = pd.to_datetime(patients_df_raw['Birthday'])

# Description day
patients_df_raw['d-day'] = pd.to_datetime(patients_df_raw['Description'])

# First Date, remove typo letters
patients_df_raw['f-day'] = pd.to_datetime(
    patients_df_raw['First Date'].replace(r"[A-Z]", "", regex=True))
```
```{python}
# f-day (first come to hospital) and d-day need to larger than birthday
# if not, delete those entries

patients_df_raw = patients_df_raw[(patients_df_raw['f-day']
                                  > patients_df_raw['b-day']) & (patients_df_raw['d-day'] > patients_df_raw['b-day'])]
```
#### New variables
In this session, new columns of `age_of_first_come` and `diagnosis-n` are created.  `age_of_first_come` is created based on the difference of Description and birthday. And `diagnosis-n` is generated by parsing the diagnosis column.
```{python}
patients_df_raw['age_of_first_come'] = patients_df_raw.apply(
    lambda row: relativedelta.relativedelta(row['d-day'], row['b-day']).years, axis=1)
# sanity check
patients_df_raw.head(5)
```
```{python}
# parsing diagnosis
diagnosis_terms = patients_df_raw['Diagnosis'].str.split(", ")
d = collections.Counter()
for phrases in diagnosis_terms:
    for phrase in phrases:
        d[phrase] += 1

sorted(d.items(), key=lambda x: x[1], reverse=True)
```
```{python}
diag = patients_df_raw['Diagnosis'].str.split(", ", expand=True)
diag.columns = ["Diagnosis"+str(i) for i in range(1, 5)]
patients_df = pd.concat([patients_df_raw, diag], axis=1)
patients_df.head(5)
```
```{python}
# save to local
patients_df[["ID", "SEX", "b-day", "d-day", "f-day", "age_of_first_come", "Admission", "Diagnosis1", "Diagnosis2", "Diagnosis3",
             "Diagnosis4"]].to_csv('../data/tsumotoa_clean.csv', index=False)
```

### TSUMOTO_B
#### load data

```{python}
# read data
slab_df_raw = pd.read_csv("../data/TSUMOTO_B.csv", encoding='windows-1252')

```
```{python}
# examine the data
slab_df_raw.head(5)
slab_df_raw.info()
```
```{python}
# values of different columns
for nm in ["KCT", "RVVT", "LAC", "ANA", "ANA Pattern"]:
    print(slab_df_raw[nm].value_counts())
    print("-------------------")
```

#### Missing values and duplicates
The dataset contains missing values for some of the columns. Thus, for different purpose, I will use different strategies to deal with missing data. This dataset allows ID has duplication, Thus, here, I only check for duplicate entries for data quality check purpose. <br>
Joining with other datasets requires ID column. Thus, for join analysis purpose, those records without ID information will be removed. 
```{python}
# deduplicate
slab_df_raw.drop_duplicates()
```
```{python}
# For join dataset
join_B = slab_df_raw.dropna(subset=['ID'])
join_B.info
```
```{python}
# save to local
join_B.to_csv('../data/tsumotob_clean.csv', index=False)
```

#### Cleaning and data format
##### Parsing Diagnosis column
The diagnosis column contains one or more diagnosis. This session is to parse the column into several key words for further analysis. 
For those thrombosis positive patient, first, using special characters (including space) parses the text, and then calculating the top 10 frequency words. The similar procedure is repeated for those thrombosis negative patient. 
```{python}
# Thrombosis positive words
positive_words = [re.split(r'[^a-zA-Z0-9/s]+', text) for text, label in
                  zip(slab_df_raw['Diagnosis'], slab_df_raw['Thrombosis']) if label >= 1 and type(text) != float]

positive_dict = collections.Counter()
for phrases in positive_words:
    positive_dict += collections.Counter(phrases)

# clean up keywords
if "" in positive_dict:
    positive_dict.pop("")

positive_top10 = sorted(positive_dict.items(),
                        key=lambda x: x[1], reverse=True)[:10]
positive_top10
```
```{python}
# Thrombosis negative words
negative_words = [re.split(r'[^a-zA-Z0-9/s]+', text) for text, label in
                  zip(slab_df_raw['Diagnosis'], slab_df_raw['Thrombosis']) if label == 0 and type(text) != float]

negative_dict = collections.Counter()
for phrases in negative_words:
    negative_dict += collections.Counter(phrases)

# clean up keywords
if "" in negative_dict:
    negative_dict.pop("")

negative_top10 = sorted(negative_dict.items(),
                        key=lambda x: x[1], reverse=True)[:10]
negative_top10
```
##### Data format
Here, I change the examination date to datetime format.
```{python}
join_B['Examination Date'] = pd.to_datetime(join_B['Examination Date'])
```
### TSUMOTO_C
#### load data

```{python}
# | warning: false
# read data
clab_df_raw = pd.read_csv("../data/TSUMOTO_C.csv",
                          encoding='ISO-8859-1', on_bad_lines='skip')

```
```{python}
# examine the data
clab_df_raw.head(5)
clab_df_raw.info()
```

#### Missing values and duplicates
The dataset contains missing values for some of the columns. All data have ID feature. Thus, it should be ok for joining analysis. Other column missing data shows below. The missing value will have different strategy to handle in EDA session. 
```{python}
# deduplicate
clab_df_raw.drop_duplicates()

```
```{python}
clab_df_raw.isna().sum()
```

#### Data format
Here, I change the date to datetime format.
```{python}
clab_df_raw['Date'] = pd.to_datetime(clab_df_raw['Date'])
```
```{python}
# measurement columns to numeric: for those don't have numeric entires, it will fill with NaN
clab_df_raw['GOT'] = pd.to_numeric(clab_df_raw['GOT'], errors='coerce')
clab_df_raw['GPT'] = pd.to_numeric(clab_df_raw['GPT'], errors='coerce')
clab_df_raw['LDH'] = pd.to_numeric(clab_df_raw['LDH'], errors='coerce')
clab_df_raw['ALP'] = pd.to_numeric(clab_df_raw['ALP'], errors='coerce')
clab_df_raw['TP'] = pd.to_numeric(clab_df_raw['TP'], errors='coerce')
clab_df_raw['ALB'] = pd.to_numeric(clab_df_raw['ALB'], errors='coerce')
clab_df_raw['UA'] = pd.to_numeric(clab_df_raw['UA'], errors='coerce')
clab_df_raw['UN'] = pd.to_numeric(clab_df_raw['UN'], errors='coerce')
clab_df_raw['CRE'] = pd.to_numeric(clab_df_raw['CRE'], errors='coerce')
clab_df_raw['T-BIL'] = pd.to_numeric(clab_df_raw['T-BIL'], errors='coerce')
clab_df_raw['T-CHO'] = pd.to_numeric(clab_df_raw['T-CHO'], errors='coerce')
clab_df_raw['TG'] = pd.to_numeric(clab_df_raw['TG'], errors='coerce')
clab_df_raw['CPK'] = pd.to_numeric(clab_df_raw['CPK'], errors='coerce')
clab_df_raw['GLU'] = pd.to_numeric(clab_df_raw['GLU'], errors='coerce')
clab_df_raw['WBC'] = pd.to_numeric(clab_df_raw['WBC'], errors='coerce')
clab_df_raw['RBC'] = pd.to_numeric(clab_df_raw['RBC'], errors='coerce')
clab_df_raw['HGB'] = pd.to_numeric(clab_df_raw['HGB'], errors='coerce')
clab_df_raw['HCT'] = pd.to_numeric(clab_df_raw['HCT'], errors='coerce')
clab_df_raw['PLT'] = pd.to_numeric(clab_df_raw['PLT'], errors='coerce')
clab_df_raw['PT'] = pd.to_numeric(clab_df_raw['PT'], errors='coerce')
clab_df_raw['APTT'] = pd.to_numeric(clab_df_raw['APTT'], errors='coerce')
clab_df_raw['FG'] = pd.to_numeric(clab_df_raw['FG'], errors='coerce')
clab_df_raw['PIC'] = pd.to_numeric(clab_df_raw['PIC'], errors='coerce')
clab_df_raw['TAT'] = pd.to_numeric(clab_df_raw['TAT'], errors='coerce')
clab_df_raw['U-PRO'] = pd.to_numeric(clab_df_raw['U-PRO'], errors='coerce')
clab_df_raw['IGG'] = pd.to_numeric(clab_df_raw['IGG'], errors='coerce')
clab_df_raw['IGA'] = pd.to_numeric(clab_df_raw['IGA'], errors='coerce')
clab_df_raw['IGM'] = pd.to_numeric(clab_df_raw['IGM'], errors='coerce')
clab_df_raw['RF'] = pd.to_numeric(clab_df_raw['RF'], errors='coerce')
clab_df_raw['C3'] = pd.to_numeric(clab_df_raw['C3'], errors='coerce')
clab_df_raw['C4'] = pd.to_numeric(clab_df_raw['C4'], errors='coerce')
clab_df_raw['DNA'] = pd.to_numeric(clab_df_raw['DNA'], errors='coerce')
clab_df_raw['DNA-II'] = pd.to_numeric(clab_df_raw['DNA-II'], errors='coerce')
```

#### New columns
For lab test, there are criteria indicating whether a indicator is out of normal range. In this session, new variable introduced based on those criteria. Note: those with gender difference will be dealt with later. 
```{python}
clab_df = clab_df_raw.copy()
clab_df.assign(is_GOT_normal=np.where(clab_df['GOT'] >= 60, 'no', np.where(clab_df['GOT'] < 60, 'yes', 'NaN')),  # is_GOT_normal N<60
               is_GPT_normal=np.where(clab_df['GPT'] >= 60, 'no', np.where(
                   clab_df['GPT'] < 60, 'yes', 'NaN')),  # is_GPT_normal N<60
               is_LDH_normal=np.where(clab_df['LDH'] >= 500, 'no', np.where(
                   clab_df['LDH'] < 500, 'yes', 'NaN')),  # is_LDH_normal N<500
               is_ALP_normal=np.where(clab_df['ALP'] >= 300, 'no', np.where(
                   clab_df['ALP'] < 300, 'yes', 'NaN')),  # is_ALP_normal N<300
               is_TP_normal=np.where(((clab_df['TP'] >= 8.5) | (clab_df['TP'] <= 6)), 'no', np.where(
                   ((clab_df['TP'] < 8.5) & (clab_df['TP'] > 6)), 'yes', 'NaN')),  # is_TP_normal 6<N<8.5
               is_ALB_normal=np.where(((clab_df['ALB'] >= 5.5) | (clab_df['ALB'] <= 3.5)), 'no', np.where(
                   ((clab_df['ALB'] < 5.5) & (clab_df['ALB'] > 3.5)), 'yes', 'NaN')),  # is_ALB_normal 3.5<N<5.5
               is_UN_normal=np.where(clab_df['UN'] <= 30, 'no', np.where(
                   clab_df['UN'] > 30, 'yes', 'NaN')),  # is_UN_normal N>30
               is_CRE_normal=np.where(clab_df['CRE'] <= 1.5, 'no', np.where(
                   clab_df['CRE'] > 1.5, 'yes', 'NaN')),  # is_CRE_normal N>1.5
               is_TBIL_normal=np.where(clab_df['T-BIL'] >= 2, 'no', np.where(
                   clab_df['T-BIL'] < 2, 'yes', 'NaN')),  # is_T-BIL_normal N<2
               is_TCHO_normal=np.where(clab_df['T-CHO'] >= 250, 'no', np.where(
                   clab_df['T-CHO'] < 250, 'yes', 'NaN')),  # is_T-CHO_normal N<250
               is_TG_normal=np.where(clab_df['TG'] >= 200, 'no', np.where(
                   clab_df['TG'] < 200, 'yes', 'NaN')),  # is_TG_normal N<200
               is_CPK_normal=np.where(clab_df['CPK'] >= 250, 'no', np.where(
                   clab_df['CPK'] < 250, 'yes', 'NaN')),  # is_CPK_normal N<250
               is_GLU_normal=np.where(clab_df['GLU'] >= 180, 'no', np.where(
                   clab_df['GLU'] < 180, 'yes', 'NaN')),  # is_GLU_normal N<180
               is_WBC_normal=np.where(((clab_df['WBC'] >= 9000) | (clab_df['WBC'] <= 3500)), 'no', np.where(
                   ((clab_df['WBC'] < 9000) & (clab_df['WBC'] > 3500)), 'yes', 'NaN')),  # is_WBC_normal 3500<N<9000
               is_RBC_normal=np.where(((clab_df['RBC'] >= 600) | (clab_df['RBC'] <= 350)), 'no', np.where(
                   ((clab_df['RBC'] < 600) & (clab_df['RBC'] > 350)), 'yes', 'NaN')),  # is_RBC_normal 350<N<600
               is_HGB_normal=np.where(((clab_df['HGB'] >= 17) | (clab_df['HGB'] <= 10)), 'no', np.where(
                   ((clab_df['HGB'] < 17) & (clab_df['HGB'] > 10)), 'yes', 'NaN')),  # is_HGB_normal 10<N<17
               is_HCT_normal=np.where(((clab_df['HCT'] >= 52) | (clab_df['HCT'] <= 29)), 'no', np.where(
                   ((clab_df['HCT'] < 52) & (clab_df['HCT'] > 29)), 'yes', 'NaN')),  # is_HGB_normal 29<N<52
               is_PLT_normal=np.where(((clab_df['PLT'] >= 400) | (clab_df['PLT'] <= 100)), 'no', np.where(
                   ((clab_df['PLT'] < 400) & (clab_df['PLT'] > 100)), 'yes', 'NaN')),  # is_PLT_normal 100<N<400
               is_PT_normal=np.where(clab_df['PT'] >= 14, 'no', np.where(
                   clab_df['PT'] < 14, 'yes', 'NaN')),  # is_PT_normal N<14
               is_APTT_normal=np.where(clab_df['APTT'] >= 45, 'no', np.where(
                   clab_df['APTT'] < 45, 'yes', 'NaN')),  # is_APTT_normal N<45
               is_FG_normal=np.where(((clab_df['FG'] >= 450) | (clab_df['FG'] <= 150)), 'no', np.where(
                   ((clab_df['FG'] < 450) & (clab_df['FG'] > 150)), 'yes', 'NaN')),  # is_FG_normal 150<N<450
               is_PIC_normal=np.where(clab_df['PIC'] >= 0.8, 'no', np.where(
                   clab_df['PIC'] < 0.8, 'yes', 'NaN')),  # is_PIC_normal N<0.8
               is_TAT_normal=np.where(clab_df['TAT'] >= 3, 'no', np.where(
                   clab_df['TAT'] < 3, 'yes', 'NaN')),  # is_TAT_normal N<3
               is_UPRO_normal=np.where(((clab_df['U-PRO'] >= 30) | (clab_df['U-PRO'] <= 0)), 'no', np.where(
                   ((clab_df['U-PRO'] < 30) & (clab_df['U-PRO'] > 0)), 'yes', 'NaN')),  # is_U-PRO_normal 0<N<30
               is_IGG_normal=np.where(((clab_df['IGG'] >= 2000) | (clab_df['IGG'] <= 900)), 'no', np.where(
                   ((clab_df['IGG'] < 2000) & (clab_df['IGG'] > 900)), 'yes', 'NaN')),  # is_IGG_normal 900<N<2000
               is_IGA_normal=np.where(((clab_df['IGA'] >= 500) | (clab_df['IGA'] <= 80)), 'no', np.where(
                   ((clab_df['IGA'] < 500) & (clab_df['IGA'] > 80)), 'yes', 'NaN')),  # is_IGA_normal 80<N<500
               is_IGM_normal=np.where(((clab_df['IGM'] >= 400) | (clab_df['IGM'] <= 40)), 'no', np.where(
                   ((clab_df['IGM'] < 400) & (clab_df['IGM'] > 40)), 'yes', 'NaN')),  # is_IGM_normal 40<N<400
               is_RF_normal=np.where(clab_df['RF'] >= 20, 'no', np.where(
                   clab_df['RF'] < 20., 'yes', 'NaN')),  # is_RF_normal N<20
               is_C3_normal=np.where(clab_df['C3'] <= 35, 'no', np.where(
                   clab_df['C3'] > 35, 'yes', 'NaN')),  # is_C3_normal N>35
               is_C4_normal=np.where(clab_df['C4'] <= 10, 'no', np.where(
                   clab_df['C4'] > 10, 'yes', 'NaN')),  # is_C4_normal N>10
               is_DNA_normal=np.where(clab_df['DNA'] >= 8, 'no', np.where(
                   clab_df['DNA'] < 8., 'yes', 'NaN')),  # is_DNA_normal N<8
               is_DNAII_normal=np.where(clab_df['DNA-II'] >= 8, 'no', np.where(
                   clab_df['DNA-II'] < 8, 'yes', 'NaN')),  # is_DNAII_normal N<8

               )

```
### Join data
For analysis using patient information, three dataset are joined to see if there are any patterns. 
```{python}
# cleaned a data
patients_df = pd.read_csv("../data/tsumotoa_clean.csv")
patients_df['b-day'] = pd.to_datetime(patients_df['b-day'])
patients_df['d-day'] = pd.to_datetime(patients_df['d-day'])
patients_df['f-day'] = pd.to_datetime(patients_df['f-day'])
# cleaned a data
slab_df = pd.read_csv("../data/tsumotob_clean.csv")
slab_df['Examination Date'] = pd.to_datetime(slab_df['Examination Date'])
# left join
join_df = clab_df.merge(patients_df, on='ID', how='left')

# outer join on b and c data
join_df = join_df.merge(slab_df, on='ID', how='outer')
```
```{python}
# date formatting
join_df['b-day'] = pd.to_datetime(join_df['b-day'])
join_df['d-day'] = pd.to_datetime(join_df['d-day'])
join_df['f-day'] = pd.to_datetime(join_df['f-day'])
join_df['Examination Date'] = pd.to_datetime(join_df['Examination Date'])
join_df['Examination Date'] = pd.to_datetime(join_df['Examination Date'])
join
```
#### Missing values
There are missing values in the data. some of there are caused by merging dataset. For example, there are patient only did special lab test but not regular, or the other way around. For now, we are not removing any of them. We will deal with missing values in EDA session with various strategies according to the features. 
```{python}
join_df.isna().sum()
```
```{python}
# Examination Date for special examination need to larger than birthday
# if not, delete those entries

join_df = join_df[(join_df['Examination Date'] > join_df['b-day'])]
```
#### New columns
`Age of special exam` is the age the patient did the special examination.  
```{python}
join_df['age_of_sexam'] = join_df.apply(
    lambda row: relativedelta.relativedelta(row['Examination Date'], row['b-day']).years, axis=1)
# sanity check
join_df.head(5)
```
`Is UA normal` depends on the gender, N > 8.0 (Male) N > 6.5 (Female). 
```{python}
join_df.assign(is_UA_normal=np.where(((join_df['UA'] <= 8) & (join_df['SEX'] == 'M')), 'no', np.where(((join_df['UA'] < 8) & (join_df['SEX'] == 'M')), 'yes', np.where(((join_df['UA'] <= 6.5) & (join_df['SEX'] == 'F')), 'no',np.where(((join_df['UA'] > 6.5) & (join_df['SEX'] == 'F')), 'yes', 'NaN'))))) # N > 8.0 (Male) N > 6.5 (Female)
```
## Exploratory analysis
### Individual datasets
#### Summary statistics of patient information
```{python}
# | label: fig-piechartsex
# | fig-cap: "Patient gender distribution"
# | fig-align: "center"

# distribution of sex
# print(patients_df['SEX'].value_counts())

# Defining colors for the pie chart
colors = ['pink', 'steelblue']

# Define the ratio of gap of each fragment in a tuple
explode = (0.05, 0.05)

# plot pie chart as grouped by sex
piechart = patients_df['SEX'].value_counts().plot(kind='pie',
                                                  autopct='%1.0f%%',
                                                  colors=colors,
                                                  explode=explode,
                                                  labels=["Female", "Male"],
                                                  fontsize=12)
plt.legend(bbox_to_anchor=(-0.05, -0.1), loc='lower left', fontsize=12)
plt.suptitle('Patient gender distribution', fontsize=16)
```

Female is the dominated gender of the patients. 

```{python}
# | label: fig-descriptionvsfirst
# | fig-cap: "Description date VS First date"
# | fig-align: "center"

plt.scatter(patients_df['d-day'], patients_df['f-day'])
plt.ylim([pd.Timestamp('1970-01-01'), pd.Timestamp('2000-12-31')])
plt.title("description date VS first date")
plt.xlabel("description date")
plt.ylabel("first date")
plt.show()
# first date always smaller than description date
# can't use one date to replace another one
```

#### Special lab information
Thrombosis level vs examination date
```{python}
# | label: fig-examinevsthrom
# | fig-cap: "Examination Date VS Thrombosis"
# | fig-align: "center"

p1 = slab_df['Examination Date'].hist(by=slab_df['Thrombosis'])
plt.show()
```
Plot keywords RELATIVE frequency
```{python}
# | label: fig-diagnosekeywords
# | fig-cap: "Thrombosis diagnosis keywords"
# | fig-align: "center"

# join keywords
key_set = {k for k, _ in positive_top10} | {k for k, _ in negative_top10}
# order by Thrombosis positive decreasing
key_ls = sorted(key_set, key=lambda x: (positive_dict[x], negative_dict[x]))

# plot keywords RELATIVE frequency
nb_positive, nb_negative = sum(
    slab_df['Thrombosis'] > 0), sum(slab_df['Thrombosis'] == 0)


hN = plt.barh(key_ls, [negative_dict[k] /
              nb_negative for k in key_ls], label='negative', color='g')
hS = plt.barh(key_ls, [-positive_dict[k] /
              nb_positive for k in key_ls], label='positive')

plt.xlim([-1, 1])
xt = plt.xticks()
n = xt[0]
s = ['%.1f' % abs(i) for i in n]
plt.xticks(n, s)
plt.legend(loc='best')
plt.axvline(0.0)
plt.show()
```
Correlation among three index in special lab results.
```{python}
# | label: fig-slabcorr
# | fig-cap: "'aCL IgG', 'aCL IgM', 'aCL IgA' Correlation"
# | fig-align: "center"

#
# # transfer by log(x+1)
# correlation could be misleading as too much 0 values
# red are Thrombosis and blue are not Thrombosis
temp = slab_df[['aCL IgG', 'aCL IgM', 'aCL IgA']].copy()
temp = temp.apply(lambda x: np.log10(x+1))

d_colors = {0: "blue", 1: "green", 2: "m", 3: "k"}
colors = [d_colors[x] for x in slab_df['Thrombosis']]

axl = pd.plotting.scatter_matrix(temp, color=colors)
```

Parallel coordinates for Thrombosis levels
```{python}
# | label: fig-slabcatecorr
# | fig-cap: "'KCT', 'RVVT', 'LAC' Correlation"
# | fig-align: "center"

KCT_present = slab_df['KCT'].apply(lambda x: type(x)) != float
RVVT_present = slab_df['RVVT'].apply(lambda x: type(x)) != float
LAC_present = slab_df['LAC'].apply(lambda x: type(x)) != float

print("There are %s records with all three indexes present." %
      sum(KCT_present & RVVT_present & LAC_present))


temp_df = slab_df[['Thrombosis', 'KCT', 'RVVT', 'LAC']
                  ][KCT_present & RVVT_present & LAC_present].copy()
plt.figure()

pd.plotting.parallel_coordinates(
    temp_df, 'Thrombosis', sort_labels=True, colormap='viridis')

plt.show()
```
The overlap of parallel coordinates plot (@fig-slabcatecorr) indicates that three indicator combination is not a good indicator of the level of Thrombosis. <br>

#### General lab information
```{python}
# | label: fig-heatmaplab
# | fig-cap: "Heatmap of general lab index"
# | fig-align: "center"

corr = clab_df.corr(method='spearman')
fig, ax = plt.subplots(figsize=(14, 12))

sns.heatmap(corr, annot=True, annot_kws={"size": 28 / np.sqrt(len(corr))})

plt.show()
```
There are some correlation in the data (@fig-heatmaplab). For example, APTT and PT has high negative correlation. FG and APTT also has correlation of 0.53. If we want to do further analysis on the numerical value of those measurements, these correlations need to be considered. 

### Join dataset
```{python}
# | label: fig-agehist
# | fig-cap: "Thrombosis VS Age"
# | fig-align: "center"

## plot age and Thrombosis level
temp = join_df.pivot(columns='Thrombosis', values='age_of_sexam')
_ = temp.plot.hist(bins=20)
```
Different level of Thrombosis have various age distribution (@fig-agehist). Regardless of number of samples, severe thrombosis happens high in 30's, while level 2 thrombosis happens during younger age. 

```{python}
# | label: fig-labcatecorr
# | fig-cap: "Parallel coordinates plot of general lab index"
# | fig-align: "center"

temp_df = join_df[['Thrombosis', 'KCT', 'RVVT', 'LAC']
                  ]
temp_df = slab_df[['Thrombosis', 'KCT', 'RVVT', 'LAC']
                  ][KCT_present & RVVT_present & LAC_present].copy()
plt.figure()

pd.plotting.parallel_coordinates(
    temp_df, 'Thrombosis', sort_labels=True, colormap='viridis')

plt.show()
```

most important symptoms -- rae's figure 6
correlation plot
based on correlation plot, pick one or two to examine the realtionship --  linear or non-linear 
also based on correlation plot, pick one or two to examine distribution differed by group (neg, pos, serv)
repeated test -- measurement distribution and change


## Final plots
most important symptoms -- rae's figure 6
correlation plot
based on correlation plot, pick one or two to examine the realtionship --  linear or non-linear 
or also based on correlation plot, pick one or two to examine distribution differed by group (neg, pos, serv)
repeated test -- measurement distribution and change

## Technical summary



```{python}
# python code
a = 1
print(a)
```
```{r}
## R code 
a = 1
print(a)
```



















